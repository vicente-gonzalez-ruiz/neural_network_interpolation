{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Notation\" data-toc-modified-id=\"Notation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Notation</a></span></li><li><span><a href=\"#Implementation\" data-toc-modified-id=\"Implementation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Implementation</a></span></li><li><span><a href=\"#Hyperparameters\" data-toc-modified-id=\"Hyperparameters-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Hyperparameters</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronal Network (NN)\n",
    "Basically, a NN is an entity that is able to learn (something) a number of dependencies between the inputs and the outputs of a system.\n",
    "\n",
    "A NN is a collection of neurons, organized in layers. In the direction of the propagation of the information, (from the input to the output of the network) the first one is called the input layer and the last one the output layer. The rest of layers are said hidden.\n",
    "\n",
    "The number of neurons in the input and the output layers, that can be any, are defined by the problem we want to address. The number of hidden layers and the number of neurons/layer depends on the complexity of the dependiencias to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notation\n",
    "* $l=0,\\cdots,L-1$ the layer index.\n",
    "* $a_i^l; i=0,\\cdots$ the activation of the $i$-th neuron of the $l$-th layer. By definition, $a^{L-1}=y$, the output activation of the NN.\n",
    "* $w_{ij}^l$ the weight that goes from the $j$-th neuron of the $l$ layer to the $i$-th neuron of the layer $l+1$. $b_{i}^l$ the bias that goes from the $i$-th neuron of the $l$ layer.\n",
    "* $e_i^l$ the scalar error of the $i$-th neuron of the $l$-th layer. By definition, the prediction error $e^{L-1}=t-y$, where $t$ is the target (ideal) output of the NN. The error in (the activation of) the internal neurons $e_i^l; l=1, \\cdots, L-2$ is estimated by using the Back-Propagation Algorithm (BPA).\n",
    "* $e_i^l=t_i-a_i^{L-1}$, the error of the $i$-th neuron of the $l$-th layer, where $t_i$ is the target value for the $i$-th component of the desired output $y$. Notice that $e^{L-1}$ is the error of the network.\n",
    "* By convenience, the $e^{L-1}$ is not directly minimized, but the cost function $c=\\sum_i e_i^2$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "Based on http://neuralnetworksanddeeplearning.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import ipdb\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, sizes=[2, 3, 1], learning_rate=1, b_init_vals=-1, w_init_vals=-1):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        \n",
    "        if b_init_vals == -1:\n",
    "            self.B = [np.random.rand(y, 1) for y in sizes[1:]]\n",
    "        else:\n",
    "            assert b_init_vals >= 0.0\n",
    "            self.B = [np.full((y, 1), b_init_vals) for y in sizes[1:]]\n",
    "        \n",
    "        if w_init_vals == -1:\n",
    "            self.W = [np.random.rand(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        else:\n",
    "            assert w_init_vals >= 0.0\n",
    "            self.W = [np.full((y, x), w_init_vals) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "        #for x, y in zip(sizes[:-1], sizes[1:]):\n",
    "        #    print (x, y)\n",
    "        self.LR = learning_rate\n",
    "\n",
    "    def feed_forward(self, a):\n",
    "        '''Propagates an activation ``a`` from the input to the output of the network.'''\n",
    "        for b, w in zip(self.B, self.W):\n",
    "            print(w.shape, a.shape)\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "    \n",
    "    def learn(self, x, t):\n",
    "        '''NN learn when the weights and biases are modified to minimize the cost function.\n",
    "        To teach a NN, tuples at least a tuple ```(x, y)``` must be presented\n",
    "        to the NN, where ```x``` is a input training example (also called a feature vector)\n",
    "        and ``t``` is the associated (ideal) output (target) that the NN should learn.\n",
    "        \n",
    "        To modify the weights and the biases, Gradient Descend Optimization (GDO) is used.\n",
    "        '''\n",
    "        \n",
    "        # Find the gradient for each weight and bias of the NN\n",
    "        nabla_b, nabla_w = self.get_gradients(x, t)\n",
    "        \n",
    "        # w_ij^l -= \\alpha/len(x)\\nabla_w_ij^l\n",
    "        self.W = [w - (self.LR/len(x))*nw for w, nw in zip(self.W, nabla_w)]\n",
    "        \n",
    "        # b_i^l -= \\alpha/len(x)\\nabla_b_i^l\n",
    "        self.B = [b - (self.LR/len(x))*nb for b, nb in zip(self.B, nabla_b)]\n",
    "\n",
    "    def cost_derivative(self, y, t):\n",
    "        \"\"\"Derivative of the cost function: 1/2*(ideal_out - out)^2.\n",
    "        Notice that the scalar has been ignored (supposed to be 1) it will be\n",
    "        considerated to be a part of the learning rate used by GDO.\"\"\"\n",
    "        return (y - t)\n",
    "\n",
    "    def get_gradients(self, _in, ideal_out):\n",
    "        \"\"\" Given an input ``_in`` and an ideal output ``ideal_out``,\n",
    "        modify the weights and biases to minimize the cost of the error.\n",
    "        \n",
    "        Backpropagate the errors from the output to the first hidden layer,\n",
    "        and computes the \n",
    "        \"\"\"\n",
    "        #ipdb.set_trace() # <-------------------------- breakpoint\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.B`` and ``self.W``.\"\"\"\n",
    "        \n",
    "        # Returns the gradient of the cost function C(x) respect to the\n",
    "        # biases (nabla_b) and weights (nabla_w).\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.B]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.W]\n",
    "        #print(len(nabla_b), len(nabla_w))\n",
    "\n",
    "        # Forward pass. We compute two lists: activations and zs,\n",
    "        # with the activations and the z's of the neurons of the network.\n",
    "        activation = _in\n",
    "        activations = [_in] # list to store all the activations, layer by layer\n",
    "        #print(_in)\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.B, self.W):\n",
    "            #for i in activations:\n",
    "            #    print(i.shape)\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "\n",
    "        # Backward pass. This is the backpropagation algorithm.\n",
    "        \n",
    "        # Derivative of the error of the cost function at the output (L-1) layer.\n",
    "        delta = self.cost_derivative(activations[-1], ideal_out) * sigmoid_derivative(zs[-1])\n",
    "        \n",
    "        # The gradient of the cost function respect to the biases at the output layer\n",
    "        # is the calculus performed in the last sentence.\n",
    "        nabla_b[-1] = delta\n",
    "        \n",
    "        # The gradient of the cost function respect to the weights at the output\n",
    "        # is the previous derivative multiplied by the activations of the L-2 layer.\n",
    "        #print(delta.shape, activations[-2].transpose().shape)\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "\n",
    "        # Now, retropropagate the error of the cost function to the rest of the \n",
    "        # layers (starting at L-2) up to the first one (layer 0), computing the gradient.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l] # Negative indexes go backwards in the list\n",
    "            sp = sigmoid_derivative(z)\n",
    "            delta = np.dot(self.W[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "\n",
    "        return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "* Input layer: $s_0$ and $s_2$, samples of $s_0, s_1, s_2, \\cdots$. \n",
    "* Output layer: $\\hat{s}_1$, a prediction.\n",
    "* Initial prediction:\n",
    "$$\n",
    "\\hat{s}_1 = \\frac{s_0 + s_2}{2}.\n",
    "$$\n",
    "* $L$ layers and number of neurons by layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 2) (2, 1)\n",
      "(16, 16) (16, 1)\n",
      "(1, 16) (16, 1)\n",
      "[[130.]]\n"
     ]
    }
   ],
   "source": [
    "net = Network(sizes=[2, 16, 16, 1], b_init_vals=-1, w_init_vals=-1)\n",
    "for i in range(2000):\n",
    "    net.learn(np.array([[10/255],[50/255]]), np.array([[130/255]]))\n",
    "print(net.feed_forward(np.array([[10/255],[50/255]])) * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 2) (2, 1)\n",
      "(16, 16) (16, 1)\n",
      "(1, 16) (16, 1)\n",
      "[[130.]]\n"
     ]
    }
   ],
   "source": [
    "net = Network(sizes=[2, 16, 16, 1], b_init_vals=0, w_init_vals=0)\n",
    "for i in range(2000):\n",
    "    net.learn(np.array([[10/255],[50/255]]), np.array([[130/255]]))\n",
    "print(net.feed_forward(np.array([[10/255],[50/255]])) * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[41, 55, 37],\n",
       "       [74, 16, 94]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randint(low=0, high=100, size=(2,3))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

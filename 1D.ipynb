{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Notation\" data-toc-modified-id=\"Notation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Notation</a></span></li><li><span><a href=\"#Implementation\" data-toc-modified-id=\"Implementation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Implementation</a></span></li><li><span><a href=\"#Hyperparameters\" data-toc-modified-id=\"Hyperparameters-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Hyperparameters</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronal Network (NN)\n",
    "Basically, a NN is an entity that is able to learn (something) a number of dependencies between the inputs and the outputs of a system.\n",
    "\n",
    "A NN is a collection of neurons, organized in layers. In the direction of the propagation of the information, (from the input to the output of the network) the first one is called the input layer and the last one the output layer. The rest of layers are said hidden.\n",
    "\n",
    "The number of neurons in the input and the output layers, that can be any, are defined by the problem we want to address. The number of hidden layers and the number of neurons/layer depends on the complexity of the dependiencias to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notation\n",
    "* $l=0,\\cdots,L-1$ the layer index.\n",
    "* $a_i^l; i=0,\\cdots$ the activation of the $i$-th neuron of the $l$-th layer.\n",
    "* $w_{ij}^l$ the weight that goes from the $j$-th neuron of the $l$ layer to the $i$-th neuron of the layer $l+1$.\n",
    "* $e_i^l=y_i-a_i^{L-1}$, the error of the $i$-th neuron of the $l$-th layer, where $y_i$ is the target value for the $i$-th component of the desired output $y$. Notice that $e^{L-1}$ is the error of the network.\n",
    "* $c=\\sum_i e_i^2$ the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "Based on http://neuralnetworksanddeeplearning.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import ipdb\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        \n",
    "        # All biases init to 0\n",
    "        self.biases = [np.zeros((y, 1)) for y in sizes[1:]]\n",
    "        \n",
    "        # All weights init to 0.5, i.e., initially we compute averages\n",
    "        self.weights = [np.full((y, x), 0.5) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        #for x, y in zip(sizes[:-1], sizes[1:]):\n",
    "        #    print (x, y)\n",
    "    \n",
    "    # Feed forward\n",
    "    def get_output(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            print(w.shape, a.shape)\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "    \n",
    "    # Gradient descend optimization on weights and biases\n",
    "    def learn(self, _in, ideal_out, learning_rate):\n",
    "        \n",
    "        # We can optimize this ...\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        #for i in self.weights:\n",
    "        #    print(i.shape)\n",
    "        delta_nabla_b, delta_nabla_w = self.backprop(_in, ideal_out)\n",
    "        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        \n",
    "        # w^l_jk -= \\alpha/len(x)\\nabla_w^_jk\n",
    "        self.weights = [w-(learning_rate/len(_in))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        \n",
    "        # b^l_k -= \\alpha/len(x)\\nabla_b^l_k\n",
    "        self.biases = [b-(learning_rate/len(_in))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "    def backprop(self, _in, ideal_out):\n",
    "        \"\"\" Given an input ``_in`` and an ideal output ``ideal_out``,\n",
    "        modify the weights and the bias to minimize the cost of the error.\n",
    "        \n",
    "        Backpropagate the errors from the output to the first hidden layer,\n",
    "        and computes the \n",
    "        \"\"\"\n",
    "        #ipdb.set_trace() # <-------------------------- breakpoint\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        \n",
    "        # Returns the gradient of the cost function C(x) respect to the\n",
    "        # biases (nabla_b) and weights (nabla_w).\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        #print(len(nabla_b), len(nabla_w))\n",
    "\n",
    "        # Forward pass. We compute two lists: activations and zs,\n",
    "        # with the activations and the z's of the neurons of the network.\n",
    "        activation = _in\n",
    "        activations = [_in] # list to store all the activations, layer by layer\n",
    "        #print(_in)\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            #for i in activations:\n",
    "            #    print(i.shape)\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "\n",
    "        # Backward pass. This is the backpropagation algorithm.\n",
    "        \n",
    "        # Derivative of the error of the cost function at the output (L-1) layer.\n",
    "        delta = self.cost_derivative(activations[-1], ideal_out) * sigmoid_derivative(zs[-1])\n",
    "        \n",
    "        # The gradient of the cost function respect to the biases at the output layer\n",
    "        # is the calculus performed in the last sentence.\n",
    "        nabla_b[-1] = delta\n",
    "        \n",
    "        # The gradient of the cost function respect to the weights at the output\n",
    "        # is the previous derivative multiplied by the activations of the L-2 layer.\n",
    "        #print(delta.shape, activations[-2].transpose().shape)\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "\n",
    "        # Now, retropropagate the error of the cost function to the rest of the \n",
    "        # layers (starting at L-2) up to the first one (layer 0), computing the gradient.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l] # Negative indexes go backwards in the list\n",
    "            sp = sigmoid_derivative(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "\n",
    "        return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "* Input layer: $s_0$ and $s_2$, samples of $s_0, s_1, s_2, \\cdots$. \n",
    "* Output layer: $\\hat{s}_1$, a prediction.\n",
    "* Initial prediction:\n",
    "$$\n",
    "\\hat{s}_1 = \\frac{s_0 + s_2}{2}.\n",
    "$$\n",
    "* $L$ layers and number of neurons by layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 2) (2, 1)\n",
      "(16, 16) (16, 1)\n",
      "(1, 16) (16, 1)\n",
      "[[30.]]\n"
     ]
    }
   ],
   "source": [
    "net = Network([2, 16, 16, 1])\n",
    "for i in range(2000):\n",
    "    net.learn(np.array([[20/255],[40/255]]), np.array([[30/255]]), 1.0)\n",
    "print(net.get_output(np.array([[20/255],[40/255]])) * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 86, 81],\n",
       "       [29, 24,  7]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randint(low=0, high=100, size=(2,3))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

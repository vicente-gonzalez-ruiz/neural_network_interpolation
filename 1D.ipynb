{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "Copia de 1D.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicente-gonzalez-ruiz/neural_network_interpolation/blob/master/1D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc": true,
        "id": "yqwWfRCCp8lw",
        "colab_type": "text"
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Neuronal-Network-(NN)\" data-toc-modified-id=\"Neuronal-Network-(NN)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Neuronal Network (NN)</a></span></li><li><span><a href=\"#Structure-description\" data-toc-modified-id=\"Structure-description-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Structure description</a></span></li><li><span><a href=\"#Algorithms\" data-toc-modified-id=\"Algorithms-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Algorithms</a></span><ul class=\"toc-item\"><li><span><a href=\"#Forward-propagation-of-the-input-activation:-the-Feed-Forward-Algorithm-(FFA)\" data-toc-modified-id=\"Forward-propagation-of-the-input-activation:-the-Feed-Forward-Algorithm-(FFA)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Forward propagation of the input activation: the Feed-Forward Algorithm (FFA)</a></span></li><li><span><a href=\"#Error-retropropagation:-the-Back-Propagation-Algorithm-(BPA)\" data-toc-modified-id=\"Error-retropropagation:-the-Back-Propagation-Algorithm-(BPA)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Error retropropagation: the Back-Propagation Algorithm (BPA)</a></span></li><li><span><a href=\"#Computation-of-the-gradient-using-Back-propagation-(BPA)-of-the-prediction-error\" data-toc-modified-id=\"Computation-of-the-gradient-using-Back-propagation-(BPA)-of-the-prediction-error-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Computation of the gradient using Back-propagation (BPA) of the prediction error</a></span></li></ul></li><li><span><a href=\"#Implementation\" data-toc-modified-id=\"Implementation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Implementation</a></span></li><li><span><a href=\"#Hyperparameters\" data-toc-modified-id=\"Hyperparameters-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Hyperparameters</a></span></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN7y05_Wp8l0",
        "colab_type": "text"
      },
      "source": [
        "# What's a Neuronal Network (NN)\n",
        "Basically, a NN is an collection of *neurons* that are able to learn *dependencies* between the inputs and the outputs of a system. Such dependencies are expressed as collections of activations (excitation levels) of the output of the neurons.\n",
        "\n",
        "In NNs the neurons are organized in *layers*. In the direction of the propagation of the information (from the input to the output of the network, and in this case, we are working with *feed-forward NNs*), the first one is called the *input layer* and the last one the output layer. The rest of layers are said hidden.\n",
        "\n",
        "Between layers, the neurons are fully interconnected by means of a collection of weights. The \"knowledge\" learned by the NN is stored in such interconnections. Moreover, each neuron has also a special unconnected input (the bias) that, depending on its vaule, inhibits or excites the neuron. \n",
        "\n",
        "The number of neurons in the input and the output layers, that can be any, are defined by the problem we want to address. For example, in classification problems, the number our output neurons usually equals the number of classes, or at least, the output is encoded as binary combinatios. However, in prediction problems, it can enough to use only one neuron per dimension and quantify the output of each (output) neuron (notice that, typically, in the classification case, the output is simply thresholded). On the other hand, the number of hidden layers and the number of neurons/layer depends on the complexity of the dependiencias to learn.\n",
        "\n",
        "In order to avoid overflow and/or underflow, inputs and targets must be numbers in $(0.0, 1.0)$ (open intervals). The output of the NN will be also in this interval of values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoJBwbs9TX2g",
        "colab_type": "text"
      },
      "source": [
        "# Test and training data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuWh1NNpp8l4",
        "colab_type": "text"
      },
      "source": [
        "# Definitions\n",
        "\n",
        "Let:\n",
        "\n",
        "* $l=1,\\cdots,L$, the layer number. $1$ is the number of the input layer and $L$ the number of the output layer.\n",
        "\n",
        "* $n^l$, the number of neurons of the $l$-th layer.\n",
        "\n",
        "* $A^l_{i=1, \\cdots, n^l}$ the *activation* (the excitation level) of the $i$-th neuron of the $l$-th layer. In consequence, following this representation, $A^L$ would be the output activation (usually a vector) of the NN, and $A^1$ the input of the NN (the activation of the neurons of the input layer).\n",
        "\n",
        "* $W^{l=2,\\cdots,L}_{ij}$ the *weight* that goes from the $i$-th neuron of the $(l-1)$-th layer to the $j$-th neuron of the layer $l$. Similarly, $B^l_j$ is the bias that inputs to the $j$-th neuron of the $l$ layer. Notice that the input layer has not weights associated with it.\n",
        "\n",
        "* $E^{l=2,\\cdots,L}_i$ the prediction error of the $i$-th neuron of the $l$-th layer (logically, the input layer can not be erroneus). The prediction error at the output layer is\n",
        "\\begin{equation}\n",
        "  E^L(A^1) := \\frac{1}{2}(T(A^1)-A^L(A^1))^2,\n",
        "\\end{equation}\n",
        "where the pair $(A^1, T(A^1))$ constitutes a training example. $T(A^1)$ is the target (ideal output) of the NN associated to the input $A^1$ (also called *feature vector*). The errors in (the activation of) the internal neurons $E^{l=2,\\cdots,L-1}_i$ will be estimated with the Back-Propagation Algorithm (BPA).\n",
        "\n",
        "* \n",
        "\n",
        "* $e_i^l=t_i-a_i^{L-1}$, the error of the $i$-th neuron of the $l$-th layer, where $t_i$ is the target value for the $i$-th component of the desired output $y$. Notice that $e^{L-1}$ is the error of the network.\n",
        "* By convenience, the $e^{L-1}$ is not directly minimized, but the cost function $c=\\sum_i e_i^2$ ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh8c6iqSTi6T",
        "colab_type": "text"
      },
      "source": [
        "# Objective\n",
        "The objetive of the NN is to minimize\n",
        "$$\n",
        "c(W,B)[i] := \\frac{1}{2n}\\sum_i^t||E^L(A^1[i])||^2,\n",
        "$$\n",
        "a *objective* function (also called *cost* and *loss* function), as a function of the weights and biases of the NN. As we can see, is $c$ is basically based on the [MSE](https://en.wikipedia.org/wiki/Mean_squared_error), where $t$ is the number of training examples, $||v||$ donotes the [magnitude](https://en.wikipedia.org/wiki/Magnitude_(mathematics)) (or length) of the vector $v$ in the Euclidean space, and $E^L(A^L[i])$ the corresponding NN's output.\n",
        "\n",
        "Therefore,\n",
        "\\begin{equation}\n",
        "\\nabla c = \\frac{1}{t}\\sum\n",
        "\\end{equation}\n",
        "\n",
        " to compute $\\nabla c$, we need to compute $\\nabla c[i]$\n",
        "\n",
        "\n",
        "Notice that the objective function must be continuous in order to be minimized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEJKlj-np8l7",
        "colab_type": "text"
      },
      "source": [
        "# Algorithms\n",
        "\n",
        "## Forward propagation of the input activation: the Feed-Forward Algorithm (FFA)\n",
        "\n",
        "FFA controls how the network propagates the input activation towards the output layer:\n",
        "1. Set $A^1=X$, where $X$ is the input of the NN.\n",
        "2. For $l=2, \\cdots, L$:\n",
        "    1. $A^l = W^l\\cdot A^{l-1}$\n",
        "\n",
        "## Prediction error retropropagation: the Back-Propagation Algorithm (BPA)\n",
        "\n",
        "The BPA estimates the prediction error in the internal neurons by retropropagating $E^L$ from the output to the input of the NN (for the rest, we will ignore that this error changes if the feature vector changes because the algorithm does not depend on that):\n",
        "1. For $l=L,\\cdots, 3$:\n",
        "    1. $E^{l-1}_i = \\sum_j^{n^l}W^l_{ij}E^l_j$\n",
        "\n",
        "## Weights and bias optimization using a Gradient Descend Algorithm (GDA)\n",
        "\n",
        "Given an input feature vector, to minimize the prediction errors $E$, we use a GDA, which is based on the calculus of the gradient of the prediction error respect to each weight and bias of the NN. For that, we must find the partial derivative\n",
        "$$\n",
        " \\frac{\\partial E^L}{\\partial W^l_{ij}},\n",
        "$$\n",
        "and move $W^l_{ij}$ in the opposite direction, iteratively. Notice that this minimization should be performed considering the total cost for all the training inputs.\n",
        "\n",
        "BPA computes the derivative of the prediction error at each neuron of the NN.\n",
        "$E^L=\\sum_k^{n^L}(T_k-A^L_k)^2$.\n",
        "\n",
        "$\n",
        "\\frac{\\partial E^L}{\\partial W^L_{ij}} = \n",
        "\\frac{\\partial    }{\\partial W^L_{ij}} \\sum_k^{n^L}(T_k-A^L_k)^2 = [\\text{by the topology of the NN}] = \n",
        "\\frac{\\partial    }{\\partial W^L_{ij}}(T_j-A^L_j)^2 = \n",
        "2(A^L_j-T_j)\\frac{\\partial A^L_j}{\\partial W^L_{ij}}\n",
        "$\n",
        "\n",
        "\n",
        "## Computation of the gradient using Back-propagation (BPA) of the prediction error\n",
        "\n",
        "Determines the computation of the error at the internal nodes of the NN:\n",
        "\n",
        "1. Set $E^L = \n",
        "$E^l_i = \\sum_j^{n^{l+1}} E^{l+1}_j W^{l+1}_{ij}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cZwZR42JOw4",
        "colab_type": "text"
      },
      "source": [
        "Activation function!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_Rexogcp8mV",
        "colab_type": "text"
      },
      "source": [
        "# Implementation\n",
        "Based on http://neuralnetworksanddeeplearning.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO8GMM5Xp8mW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "#import ipdb\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"The sigmoid function.\"\"\"\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        "\n",
        "class Network:\n",
        "\n",
        "    def __init__(self, sizes=[2, 3, 1], learning_rate=1, initial_biases=-1):\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        \n",
        "        if initial_biases == -1:\n",
        "            if __debug__:\n",
        "                print(\"Randomizing biases using uniform [0.01, 0.99]\")\n",
        "            self.B = [np.random.uniform(low=0.01, high=0.99, size=(y, 1)) for y in sizes[1:]]\n",
        "        else:\n",
        "            if __debug__:\n",
        "                print(f\"All biases initialized to {initial_biases}\")\n",
        "            assert initial_biases >= 0.0\n",
        "            self.B = [np.full((n_l, 1), initial_biases) for n_l in sizes[1:]]\n",
        "            \n",
        "        self.W = [np.random.normal(loc=0.0, scale=pow(n_l, -0.5), size=(n_l, n_l_1))\n",
        "                  for n_l_1, n_l in zip(sizes[:-1], sizes[1:])]\n",
        "        for W_l in self.W:\n",
        "            W_l = (W_l-W_l.min()) / (W_l.max() - W_l.min())\n",
        "\n",
        "        self.LR = learning_rate\n",
        "\n",
        "    def feed_forward(self, a):\n",
        "        '''Propagates an activation ``a`` from the input to the output of the network.'''\n",
        "        for b, w in zip(self.B, self.W):\n",
        "            print(w.shape, a.shape)\n",
        "            a = sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "    \n",
        "    def learn(self, x, t):\n",
        "        '''NN learn when the weights and biases are modified to minimize the cost function.\n",
        "        To teach a NN, tuples at least a tuple ```(x, y)``` must be presented\n",
        "        to the NN, where ```x``` is a input training example (also called a feature vector)\n",
        "        and ``t``` is the associated (ideal) output (target) that the NN should learn.\n",
        "        \n",
        "        To modify the weights and the biases, Gradient Descend Optimization (GDO) is used.\n",
        "        '''\n",
        "        \n",
        "        # Find the gradient for each weight and bias of the NN\n",
        "        nabla_b, nabla_w = self.get_gradients(x, t)\n",
        "        \n",
        "        # w_ij^l -= \\alpha/len(x)\\nabla_w_ij^l\n",
        "        self.W = [w - (self.LR/len(x))*nw for w, nw in zip(self.W, nabla_w)]\n",
        "        \n",
        "        # b_i^l -= \\alpha/len(x)\\nabla_b_i^l\n",
        "        self.B = [b - (self.LR/len(x))*nb for b, nb in zip(self.B, nabla_b)]\n",
        "\n",
        "    def cost_derivative(self, A_at_L, target):\n",
        "        \"\"\"Derivative of the cost function: 1/2*(ideal_out - out)^2.\n",
        "        Notice that the scalar has been ignored (supposed to be 1) it will be\n",
        "        considerated to be a part of the learning rate used by GDO.\"\"\"\n",
        "        return (A_at_L - target)\n",
        "\n",
        "    def get_gradients(self, _in, ideal_out):\n",
        "        \"\"\" Given an input ``_in`` and an ideal output ``ideal_out``,\n",
        "        modify the weights and biases to minimize the cost of the error.\n",
        "        \n",
        "        Backpropagate the errors from the output to the first hidden layer,\n",
        "        and computes the \n",
        "        \"\"\"\n",
        "        #ipdb.set_trace() # <-------------------------- breakpoint\n",
        "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
        "        gradient for the cost function C_x.  ``nabla_b`` and\n",
        "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
        "        to ``self.B`` and ``self.W``.\"\"\"\n",
        "        \n",
        "        # Returns the gradient of the cost function C(x) respect to the\n",
        "        # biases (nabla_b) and weights (nabla_w).\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.B]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.W]\n",
        "        #print(len(nabla_b), len(nabla_w))\n",
        "\n",
        "        # Forward pass. We compute two lists: activations and zs,\n",
        "        # with the activations and the z's of the neurons of the network.\n",
        "        activation = _in\n",
        "        activations = [_in] # list to store all the activations, layer by layer\n",
        "        #print(_in)\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.B, self.W):\n",
        "            #for i in activations:\n",
        "            #    print(i.shape)\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # Backward pass. This is the backpropagation algorithm.\n",
        "        \n",
        "        # Derivative of the error of the cost function at the output (L-1) layer.\n",
        "        delta = self.cost_derivative(activations[-1], ideal_out) * sigmoid_derivative(zs[-1])\n",
        "        \n",
        "        # The gradient of the cost function respect to the biases at the output layer\n",
        "        # is the calculus performed in the last sentence.\n",
        "        nabla_b[-1] = delta\n",
        "        \n",
        "        # The gradient of the cost function respect to the weights at the output\n",
        "        # is the previous derivative multiplied by the activations of the L-2 layer.\n",
        "        #print(delta.shape, activations[-2].transpose().shape)\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "        # Now, retropropagate the error of the cost function to the rest of the \n",
        "        # layers (starting at L-2) up to the first one (layer 0), computing the gradient.\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l] # Negative indexes go backwards in the list\n",
        "            sp = sigmoid_derivative(z)\n",
        "            delta = np.dot(self.W[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "\n",
        "        return (nabla_b, nabla_w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44fyb6sqp8mh",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "* Input layer: $s_0$ and $s_2$, samples of $s_0, s_1, s_2, \\cdots$. \n",
        "* Output layer: $\\hat{s}_1$, a prediction.\n",
        "* Initial prediction:\n",
        "$$\n",
        "\\hat{s}_1 = \\frac{s_0 + s_2}{2}.\n",
        "$$\n",
        "* $L$ layers and number of neurons by layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wayPHdNqp8mj",
        "colab_type": "code",
        "outputId": "4ba4bc71-81b6-45a0-fecb-5b872c1505a3",
        "colab": {}
      },
      "source": [
        "net = Network(sizes=[2, 16, 16, 1], initial_biases=-1)\n",
        "for i in range(2000):\n",
        "    net.learn(np.array([[10/255],[50/255]]), np.array([[130/255]]))\n",
        "print(net.feed_forward(np.array([[10/255],[50/255]])) * 255)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Randomizing biases using uniform [0.01, 0.99]\n",
            "(16, 2) (2, 1)\n",
            "(16, 16) (16, 1)\n",
            "(1, 16) (16, 1)\n",
            "[[130.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsjTnBGLp8mv",
        "colab_type": "code",
        "outputId": "22716763-355f-40b6-df51-6986d8758a7a",
        "colab": {}
      },
      "source": [
        "net = Network(sizes=[2, 16, 16, 1], initial_biases=0)\n",
        "for i in range(2000):\n",
        "    net.learn(np.array([[10/255],[50/255]]), np.array([[130/255]]))\n",
        "print(net.feed_forward(np.array([[10/255],[50/255]])) * 255)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Intializing biases to 0\n",
            "(16, 2) (2, 1)\n",
            "(16, 16) (16, 1)\n",
            "(1, 16) (16, 1)\n",
            "[[130.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqHwpEjGp8m5",
        "colab_type": "code",
        "outputId": "6fe7ca47-d092-4163-cbb8-05fcee09fef6",
        "colab": {}
      },
      "source": [
        "x = np.random.randint(low=0, high=100, size=(2,3))\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[41, 55, 37],\n",
              "       [74, 16, 94]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--2AC7z_p8nO",
        "colab_type": "code",
        "outputId": "aa6de4f5-1405-4b47-9b0d-85ed9f7ab7ab",
        "colab": {}
      },
      "source": [
        "x[1,0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJnWKu8gp8nV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
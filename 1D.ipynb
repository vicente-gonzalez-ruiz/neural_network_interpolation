{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Hyperparameters\" data-toc-modified-id=\"Hyperparameters-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Hyperparameters</a></span></li><li><span><a href=\"#Implementation-based-on-http://neuralnetworksanddeeplearning.com\" data-toc-modified-id=\"Implementation-based-on-http://neuralnetworksanddeeplearning.com-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Implementation based on <a href=\"http://neuralnetworksanddeeplearning.com\" target=\"_blank\">http://neuralnetworksanddeeplearning.com</a></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "* Input layer: $s_0$ and $s_2$, samples of $s_0, s_1, s_2, \\cdots$. \n",
    "* Output layer: $\\hat{s}_1$, a prediction.\n",
    "* Initial prediction:\n",
    "$$\n",
    "\\hat{s}_1 = \\frac{s_0 + s_2}{2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation based on http://neuralnetworksanddeeplearning.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import ipdb\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        \n",
    "        # All biases init to 0\n",
    "        self.biases = [np.zeros((y, 1)) for y in sizes[1:]]\n",
    "        \n",
    "        # All weights init to 0.5, i.e., initially we compute averages\n",
    "        self.weights = [np.full((y, x), 0.5) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        #for x, y in zip(sizes[:-1], sizes[1:]):\n",
    "        #    print (x, y)\n",
    "    \n",
    "    def get_output(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "    \n",
    "    # Gradient descend optimization on weights and biases\n",
    "    def learn(self, _in, ideal_out, learning_rate):\n",
    "        \n",
    "        # We can optimize this ...\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        #for i in self.weights:\n",
    "        #    print(i.shape)\n",
    "        delta_nabla_b, delta_nabla_w = self.backprop(_in, ideal_out)\n",
    "        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        \n",
    "        # w^l_jk -= \\alpha/len(x)\\nabla_w^_jk\n",
    "        self.weights = [w-(learning_rate/len(_in))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        \n",
    "        # b^l_k -= \\alpha/len(x)\\nabla_b^l_k\n",
    "        self.biases = [b-(learning_rate/len(_in))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "    def backprop(self, _in, ideal_out):\n",
    "        #ipdb.set_trace() # <-------------------------- breakpoint\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        \n",
    "        # Returns the gradient of the cost function C(x) respect to the\n",
    "        # biases (nabla_b) and weights (nabla_w).\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        #print(len(nabla_b), len(nabla_w))\n",
    "\n",
    "        # Forward pass. We compute two lists: activations and zs,\n",
    "        # with the activations and the z's of the neurons of the network.\n",
    "        activation = _in\n",
    "        activations = [_in] # list to store all the activations, layer by layer\n",
    "        #print(_in)\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            #for i in activations:\n",
    "            #    print(i.shape)\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "\n",
    "        # Backward pass. This is the backpropagation algorithm.\n",
    "        \n",
    "        # Derivative of the error of the cost function at the output (L-1) layer.\n",
    "        delta = self.cost_derivative(activations[-1], ideal_out) * sigmoid_derivative(zs[-1])\n",
    "        \n",
    "        # The gradient of the cost function respect to the biases at the output layer\n",
    "        # is the calculus performed in the last sentence.\n",
    "        nabla_b[-1] = delta\n",
    "        \n",
    "        # The gradient of the cost function respect to the weights at the output\n",
    "        # is the previous derivative multiplied by the activations of the L-2 layer.\n",
    "        #print(delta.shape, activations[-2].transpose().shape)\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "\n",
    "        # Now, retropropagate the error of the cost function to the rest of the \n",
    "        # layers (starting at L-2) up to the first one (layer 0), computing the gradient.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l] # Negative indexes go backwards in the list\n",
    "            sp = sigmoid_derivative(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "\n",
    "        return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30.]]\n"
     ]
    }
   ],
   "source": [
    "net = Network([2, 16, 16, 1])\n",
    "for i in range(2000):\n",
    "    net.learn(np.array([[20/255],[40/255]]), np.array([[30/255]]), 1.0)\n",
    "print(net.get_output(np.array([[20/255],[40/255]])) * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vicente-gonzalez-ruiz/neural_network_interpolation/blob/master/1D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yqwWfRCCp8lw",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Neuronal-Network-(NN)\" data-toc-modified-id=\"Neuronal-Network-(NN)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Neuronal Network (NN)</a></span></li><li><span><a href=\"#Structure-description\" data-toc-modified-id=\"Structure-description-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Structure description</a></span></li><li><span><a href=\"#Algorithms\" data-toc-modified-id=\"Algorithms-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Algorithms</a></span><ul class=\"toc-item\"><li><span><a href=\"#Forward-propagation-of-the-input-activation:-the-Feed-Forward-Algorithm-(FFA)\" data-toc-modified-id=\"Forward-propagation-of-the-input-activation:-the-Feed-Forward-Algorithm-(FFA)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Forward propagation of the input activation: the Feed-Forward Algorithm (FFA)</a></span></li><li><span><a href=\"#Error-retropropagation:-the-Back-Propagation-Algorithm-(BPA)\" data-toc-modified-id=\"Error-retropropagation:-the-Back-Propagation-Algorithm-(BPA)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Error retropropagation: the Back-Propagation Algorithm (BPA)</a></span></li><li><span><a href=\"#Computation-of-the-gradient-using-Back-propagation-(BPA)-of-the-prediction-error\" data-toc-modified-id=\"Computation-of-the-gradient-using-Back-propagation-(BPA)-of-the-prediction-error-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Computation of the gradient using Back-propagation (BPA) of the prediction error</a></span></li></ul></li><li><span><a href=\"#Implementation\" data-toc-modified-id=\"Implementation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Implementation</a></span></li><li><span><a href=\"#Hyperparameters\" data-toc-modified-id=\"Hyperparameters-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Hyperparameters</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EN7y05_Wp8l0"
   },
   "source": [
    "# What's a Neuronal Network (NN)\n",
    "Basically, a NN is an collection of *neurons* that are able to learn *dependencies* between the inputs and the outputs of a system. Such dependencies are expressed as collections of activations (excitation levels) of the output of the neurons. From a pure mathematical point of view, a NN try to approximate an unknown function $F(X)$, for all $X$.\n",
    "\n",
    "In NNs the neurons are organized in *layers*. In the direction of the propagation of the information (from the input to the output of the network, and in this case, we are working with *feed-forward NNs*), the first one is called the *input layer* and the last one the output layer. The rest of layers are said hidden.\n",
    "\n",
    "Between layers, the neurons are fully interconnected by means of a collection of weights. The \"knowledge\" learned by the NN is stored in such interconnections. Moreover, each neuron has also a special unconnected input (the bias) that, depending on its vaule, inhibits or excites the neuron. \n",
    "\n",
    "The number of neurons in the input and the output layers, that can be any, are defined by the problem we want to address. For example, in classification problems, the number our output neurons usually equals the number of classes, or at least, the output is encoded as binary combinatios. However, in prediction problems, it can enough to use only one neuron per dimension and quantify the output of each (output) neuron (notice that, typically, in the classification case, the output is simply thresholded). On the other hand, the number of hidden layers and the number of neurons/layer depends on the complexity of the dependiencias to learn.\n",
    "\n",
    "In order to avoid overflow and/or underflow, inputs and targets must be numbers in $(0.0, 1.0)$ (open intervals). The output of the NN will be also in this interval of values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eteeEnYI3gna"
   },
   "source": [
    "## Topology\n",
    "Let:\n",
    "* $l=1,\\cdots,l_\\text{out}$, the layer number. $1$ is the number of the input layer and $l_\\text{out}$ the number of the output layer.\n",
    "\n",
    "* $n^l$, the number of neurons of the $l$-th layer.\n",
    "\n",
    "Al layers are fully interconnected between them by links that are wighted. Let:\n",
    "\n",
    "* $W^{l=2,\\cdots,l_\\text{out}}_{i,j}$ the *weight* that goes from the $j$-th neuron of the $(l-1)$-th layer to the $i$-th neuron of the layer $l$. Similarly, $B^l_j$ is the bias that inputs to the $j$-th neuron of the $l$ layer. Notice that the input layer has not weights associated with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9n_kk6T8Z0k"
   },
   "source": [
    "## Forward propagation\n",
    "\n",
    "Let:\n",
    "\n",
    "* $A^l_{i=1, \\cdots, n^l}$ the *activation* (the excitation level) of the $i$-th neuron of the $l$-th layer. In consequence, following this representation, $A^{l_\\text{out}}$ would be the activation (usually a vector) of the output layer of the NN, and $A^1$ the input of the NN (the activation of the neurons of the input layer).\n",
    "\n",
    "The forward propagation propagates the input activation towards the output layer:\n",
    "1. Set $A^1=X$, where $X$ is an input of the NN.\n",
    "2. For $l=2, \\cdots, l_\\text{out}$:\n",
    "    1. $Z^l = W^l\\cdot A^{l-1} + B^l$  /* Notice: $Z^l_i = \\sum_j^{n^{l-1}}W^l_{i,j}A^{l-1}_j + B^l_j$ */\n",
    "    2. $A^l = \\sigma(Z^l)$ \n",
    "\n",
    "Where $\\sigma$ is the activation function. $Z$ are the *wighted inputs* of the neurons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKDuiATw56aU"
   },
   "source": [
    "## Training\n",
    "A NN learns by minimizing the cost function\n",
    "\\begin{equation}\n",
    " c(W,B) := \\frac{1}{n}\\sum_{\\{X\\}}c_X(W,B)\n",
    "\\end{equation}\n",
    "as a function of the weights and biases (the adquired \"knowledge\") of the NN, where $n$ is the number training examples, and\n",
    "\\begin{equation}\n",
    " c_X(W,B) = \\frac{1}{2}||F(X)-A^{l_\\text{out}}||^2,\n",
    "\\end{equation}\n",
    "where $||v||$ donotes the [magnitude](https://en.wikipedia.org/wiki/Magnitude_(mathematics)) (or length) of the vector $v$ in the Euclidean space, and $\\{X\\}$ is the set of training inputs.\n",
    "\n",
    "As we can see, $c$ is a scaled version of the [MSE](https://en.wikipedia.org/wiki/Mean_squared_error), which allow us to deal with positive and negative prediction errors. Notice that $c$ must be continuous in order to be minimized.\n",
    "\n",
    "Due to high number of weights and biases, Gradient Descend is used for minimizing $c$. This means that we must compute\n",
    "\\begin{equation}\n",
    " \\nabla c = \\frac{1}{\\text{size}\\{X\\}}\\sum_{\\{X\\}}\\nabla c_X,\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "  \\nabla c_X =\n",
    "  \\begin{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "    \\frac{\\partial c_X}{\\partial W_{1,1}^2} & \\frac{\\partial c_X}{\\partial W_{1,2}^2} & \\cdots & \\frac{\\partial c_X}{\\partial W_{1,n^1}^2} \\\\\n",
    "    \\frac{\\partial c_X}{\\partial W_{2,1}^2} & \\frac{\\partial c_X}{\\partial W_{2,2}^2} & \\cdots & \\frac{\\partial c_X}{\\partial W_{2,n^1}^2} \\\\\n",
    "    \\vdots & \\vdots & & \\vdots \\\\\n",
    "    \\frac{\\partial c_X}{\\partial W_{n^2,1}^2} & \\frac{\\partial c_X}{\\partial W_{n^2,2}^2} & \\cdots & \\frac{\\partial c_X}{\\partial W_{n^2,n^1}^2}\n",
    "  \\end{bmatrix},\\cdots,\n",
    "  \\begin{bmatrix}\n",
    "    \\frac{\\partial c_X}{\\partial W_{1,1}^{l_\\text{out}}} & \\frac{\\partial c_X}{\\partial W_{1,2}^{l_\\text{out}}} & \\cdots & \\frac{\\partial c_X}{\\partial W_{1,n^\\text{out}-1}^{l_\\text{out}}} \\\\\n",
    "    \\frac{\\partial c_X}{\\partial W_{2,1}^{l_\\text{out}}} & \\frac{\\partial c_X}{\\partial W_{2,2}^{l_\\text{out}}} & \\cdots & \\frac{\\partial c_X}{\\partial W_{2,n^\\text{out-1}}^{l_\\text{out}}} \\\\\n",
    "    \\vdots & \\vdots & & \\vdots \\\\\n",
    "    \\frac{\\partial c_X}{\\partial W_{n^{\\text{out}-1},1}^{l_\\text{out}}} & \\frac{\\partial c_X}{\\partial W_{n^{\\text{out}-1},2}^{l_\\text{out}}} & \\cdots & \\frac{\\partial c_X}{\\partial W_{n^{\\text{out}},n^\\text{out}-1}^{l_\\text{out}}} \\\\\n",
    "  \\end{bmatrix},\\\\\n",
    "    \\begin{bmatrix}\n",
    "     \\frac{\\partial c_X}{\\partial B_1^2} \\\\\n",
    "     \\frac{\\partial c_X}{\\partial B_2^2} \\\\\n",
    "     \\vdots \\\\\n",
    "    \\frac{\\partial c_X}{\\partial B_{n^2}^2} \n",
    "  \\end{bmatrix},\\cdots,\n",
    "  \\begin{bmatrix}\n",
    "    \\frac{\\partial c_X}{\\partial B_1^{l_\\text{out}}} \\\\\n",
    "    \\frac{\\partial c_X}{\\partial B_2^{l_\\text{out}}} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial c_X}{\\partial B_{n^\\text{out}}^{l_\\text{out}}}\n",
    "  \\end{bmatrix}\n",
    "  \\end{bmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "$\\nabla c_X$ is a list of matrices (weights) and column vectors (biases) that expresses how quicly the cost changes when we modify the weights and the biases.\n",
    "\n",
    "[Note that we have supposed that $n^2=n^3=\\cdots=n^{l_\\text{out}}$. If this is not true, there will be $0$-entries in the gradient matrix.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbBfsPZSERW7"
   },
   "source": [
    "## Back-Propagation\n",
    "To compute $\\nabla c_X$, i.e., we need to know how $c$ changes with respect to $W$ and $B$ for a given input $X$. Notice that, to known how $c$ changes for the complete trainig set $\\{X\\}$ (that, or course, can be a [minibatch](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/)), we will compute the average of the gradients.\n",
    "\n",
    "Let's start with the weights (the computation of the gradient for the biases can be derived from the expression that we are going to obtain, considering that the activation connected to such biases are $[1 1 \\cdots 1]^T$.\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial c_X}{\\partial W^{l_\\text{out}}_{k,j}} =\n",
    "  \\frac{\\partial}{\\partial W^{l_\\text{out}}_{k,j}} \\sum_i^{n^{l_\\text{out}}}\n",
    "    \\frac{1}{2}(F(X)_i - A_i^{l_\\text{out}})^2 =\n",
    "  -(F(X)_k - A_k^{l_\\text{out}}) \\frac{\\partial A_k^{l_\\text{out}}}{\\partial W^{l_\\text{out}}_{k,j}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial A_k^{l_\\text{out}}}{\\partial W^{l_\\text{out}}_{k,j}} =\n",
    "  \\frac{\\partial}{\\partial W^{l_\\text{out}}_{k,j}}\\sigma(Z_k^{l_\\text{out}}) =\n",
    "  \\sigma(Z_k^{l_\\text{out}})(1-\\sigma(Z_k^{l_\\text{out}}))\\frac{\\partial Z^{l_\\text{out}}}{\\partial W^{l_\\text{out}}_{k,j}},\n",
    "\\end{equation}\n",
    "where, in general,\n",
    "\\begin{equation}\n",
    "  Z_k^l = \\sum_j^{n^l}A_j^{l-1}W_{k,j}^l.\n",
    "\\end{equation}\n",
    "\n",
    "Finally,\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial Z_k^{l_\\text{out}}}{\\partial W^{l_\\text{out}}_{k,j}} = A_i^{l_\\text{out}-1}.\n",
    "\\end{equation}\n",
    "\n",
    "Therefore,\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial c_X}{\\partial W^{l_\\text{out}}_{k,j}} =\n",
    "  -(F(X)_k-A_k^{l_\\text{out}})\\sigma(Z_k^{l_\\text{out}})(1-\\sigma(Z_k^{l_\\text{out}}))A_i^{l_\\text{out}-1}.\n",
    "\\end{equation}\n",
    "\n",
    "Now, lets call\n",
    "\\begin{equation}\n",
    "  -(F(X)_k-A_k^{l_\\text{out}})\\sigma(Z_k^{l_\\text{out}})(1-\\sigma(Z_k^{l_\\text{out}})) =\n",
    "  E_k^{l_\\text{out}}.\n",
    "\\end{equation}\n",
    "\n",
    "As we can see, it's holds also that\n",
    "\\begin{equation}\n",
    "  E_k^{l_\\text{out}} = \\frac{\\partial c_X}{\\partial A_k^{l_\\text{out}}}\\sigma'(Z_k^{l_\\text{out}}),\n",
    "\\end{equation}\n",
    "that for a hidden layer $l$ becomes\n",
    "\\begin{equation}\n",
    "  E_k^l = \\frac{\\partial c_X}{\\partial A_k^l}\\sigma'(Z_k^l)),\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial c_X^l}{\\partial A_k^l} =\n",
    "  \\sum_i^{n^{l+1}} E_i^{l+1}W_{i,k}^{l+1}\n",
    "\\end{equation}\n",
    "is an estimation of the error at the layer $l$ respect to a change in the activation of this layer. Substituting, we get that\n",
    "\\begin{equation}\n",
    "  E_k^l = \\sum_i^{n^{l+1}} E_i^{l+1}W_{i,k}^{l+1}\\sigma'(Z_k^l)).\n",
    "\\end{equation}\n",
    "\n",
    "So, in general, for hidden layers we have that\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial c_X}{\\partial W_{k,j}} = E_k^l A_j.\n",
    "\\end{equation}\n",
    "\n",
    "To get the gradient respect to the biases, we simply must compute that\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial c_X}{\\partial B_k} = E_k^l.\n",
    "\\end{equation}\n",
    "\n",
    "<--------->\n",
    "\n",
    "Lets define the (prediction) error at the $k$-th neuron of the output layer as\n",
    "\\begin{equation}\n",
    " \\delta^{l_\\text{out}}_j := \\frac{\\partial c_X}{\\partial Z^{l_\\text{out}}_j}.\n",
    "\\end{equation}\n",
    "Substituting, we get\n",
    "\\begin{equation}\n",
    "\\delta^{l_\\text{out}}_j =\n",
    "\\frac{\\partial(\\frac{1}{2}||F(X)-A^{l_\\text{out}}||^2)}{\\partial Z^{l_\\text{out}}_j} =\n",
    "\\frac{\\partial(\\frac{1}{2}||F(X)-\\sigma(Z^{l_\\text{out}})||^2)}{\\partial Z^{l_\\text{out}}_j} =\n",
    "(F(X)-\\sigma(Z^{l_\\text{out}}))\\sigma'(Z^{l_\\text{out}}_j) =\n",
    "\\frac{\\partial c_X}{\\partial A^{l_\\text{out}}_j}\\sigma'(Z^{l_\\text{out}}_j),\n",
    "\\end{equation}\n",
    "that is the error at the output layer. Now, we retropropagate the error.\n",
    "\n",
    "For a given layer $l$, the previous expresion can be rewritten as\n",
    "\\begin{equation}\n",
    "\\delta^l_j = \\frac{\\partial c^l_X}{\\partial A^l_j}\\sigma'(Z^l_j),\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "\\frac{\\partial c^l_X}{\\partial A^l_j} = \\sum_k^{n_{l+1}} W^{l+1}_{k,j}\\delta^{l+1}_k\n",
    "\\end{equation}\n",
    "is an estimation of the change of the error at the layer $l$ respect to a change in the activation of this layer. Substituting this in the previous equation, we get\n",
    "\\begin{equation}\n",
    "\\delta^l_j = \\sigma'(Z^l_j)\\sum_k^{n_{l+1}} W^{l+1}_{k,j}\\delta^{l+1}_k.\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, BP retro-propagates the error towards the input layer:\n",
    "1. Set $\\delta^{l_\\text{out}} = \\nabla_{A^{l_\\text{out}}}c_X\\odot\\sigma'(Z^{l_\\text{out}})$\n",
    "2. For $l=l_\\text{out},\\cdots, 3$:\n",
    "    1. $\\delta^{l-1} = ((W^l)^T\\delta^l)\\odot\\sigma'(Z^{l-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients as a function of the errors\n",
    "$\\delta^l_j$ represents the error at the neuron $j$ of the layer $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rh8c6iqSTi6T"
   },
   "source": [
    "# Objective\n",
    "The objetive of the NN is to minimize\n",
    "$$\n",
    "c(W,B)[i] := \\frac{1}{2n}\\sum_i^t||E^L(A^1[i])||^2,\n",
    "$$\n",
    "a *objective* function (also called *cost* and *loss* function), as a function of the weights and biases of the NN. As we can see, is $c$ is basically based on the [MSE](https://en.wikipedia.org/wiki/Mean_squared_error), where $t$ is the number of training examples, , and $E^L(A^L[i])$ the corresponding NN's output.\n",
    "\n",
    "Therefore,\n",
    "\\begin{equation}\n",
    "\\nabla c = \\frac{1}{t}\\sum\n",
    "\\end{equation}\n",
    "\n",
    " to compute $\\nabla c$, we need to compute $\\nabla c[i]$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FEJKlj-np8l7"
   },
   "source": [
    "# Algorithms\n",
    "\n",
    "\n",
    "\n",
    "## Prediction error retropropagation: the Back-Propagation Algorithm (BPA)\n",
    "\n",
    "The BPA estimates the prediction error in the internal neurons by retropropagating $E^L$ from the output to the input of the NN (for the rest, we will ignore that this error changes if the feature vector changes because the algorithm does not depend on that):\n",
    "1. For $l=L,\\cdots, 3$:\n",
    "    1. $E^{l-1}_i = \\sum_j^{n^l}W^l_{ij}E^l_j$\n",
    "\n",
    "## Weights and bias optimization using a Gradient Descend Algorithm (GDA)\n",
    "\n",
    "Given an input feature vector, to minimize the prediction errors $E$, we use a GDA, which is based on the calculus of the gradient of the prediction error respect to each weight and bias of the NN. For that, we must find the partial derivative\n",
    "$$\n",
    " \\frac{\\partial E^L}{\\partial W^l_{ij}},\n",
    "$$\n",
    "and move $W^l_{ij}$ in the opposite direction, iteratively. Notice that this minimization should be performed considering the total cost for all the training inputs.\n",
    "\n",
    "BPA computes the derivative of the prediction error at each neuron of the NN.\n",
    "$E^L=\\sum_k^{n^L}(T_k-A^L_k)^2$.\n",
    "\n",
    "$\n",
    "\\frac{\\partial E^L}{\\partial W^L_{ij}} = \n",
    "\\frac{\\partial    }{\\partial W^L_{ij}} \\sum_k^{n^L}(T_k-A^L_k)^2 = [\\text{by the topology of the NN}] = \n",
    "\\frac{\\partial    }{\\partial W^L_{ij}}(T_j-A^L_j)^2 = \n",
    "2(A^L_j-T_j)\\frac{\\partial A^L_j}{\\partial W^L_{ij}}\n",
    "$\n",
    "\n",
    "\n",
    "## Computation of the gradient using Back-propagation (BPA) of the prediction error\n",
    "\n",
    "* $E^{l=2,\\cdots,l_\\text{out}}_i$ the prediction error of the $i$-th neuron of the $l$-th layer (logically, the input layer can not be erroneus). In order to deal with negative and positive errors, we work with a \n",
    "\\begin{equation}\n",
    "  \n",
    " he prediction error at the output layer is\n",
    "\\begin{equation}\n",
    "  E^L[A^1] := \\frac{1}{2}(T[A^1]-A^L[A^1])^2,\n",
    "\\end{equation}\n",
    "where the pair $(A^1, T[A^1])$ constitutes a training example. $T(A^1)$ is the target (ideal output) of the NN associated to the input $A^1$ (also called *feature vector*). The errors in (the activation of) the internal neurons $E^{l=2,\\cdots,L-1}_i$ will be estimated with the Back-Propagation Algorithm (BPA).\n",
    "\n",
    "\n",
    "Determines the computation of the error at the internal nodes of the NN:\n",
    "\n",
    "1. Set $E^L = \n",
    "$E^l_i = \\sum_j^{n^{l+1}} E^{l+1}_j W^{l+1}_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PuWh1NNpp8l4"
   },
   "source": [
    "# Definitions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* \n",
    "\n",
    "* $e_i^l=t_i-a_i^{L-1}$, the error of the $i$-th neuron of the $l$-th layer, where $t_i$ is the target value for the $i$-th component of the desired output $y$. Notice that $e^{L-1}$ is the error of the network.\n",
    "* By convenience, the $e^{L-1}$ is not directly minimized, but the cost function $c=\\sum_i e_i^2$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_cZwZR42JOw4"
   },
   "source": [
    "Activation function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_Rexogcp8mV"
   },
   "source": [
    "# Implementation\n",
    "Based on http://neuralnetworksanddeeplearning.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WO8GMM5Xp8mW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import ipdb\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, sizes=[2, 3, 1], learning_rate=1, initial_biases=-1):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        \n",
    "        if initial_biases == -1:\n",
    "            if __debug__:\n",
    "                print(\"Randomizing biases using uniform [0.01, 0.99]\")\n",
    "            self.B = [np.random.uniform(low=0.01, high=0.99, size=(y, 1)) for y in sizes[1:]]\n",
    "        else:\n",
    "            if __debug__:\n",
    "                print(f\"All biases initialized to {initial_biases}\")\n",
    "            assert initial_biases >= 0.0\n",
    "            self.B = [np.full((n_l, 1), initial_biases) for n_l in sizes[1:]]\n",
    "            \n",
    "        self.W = [np.random.normal(loc=0.0, scale=pow(n_l, -0.5), size=(n_l, n_l_1))\n",
    "                  for n_l_1, n_l in zip(sizes[:-1], sizes[1:])]\n",
    "        for W_l in self.W:\n",
    "            W_l = (W_l-W_l.min()) / (W_l.max() - W_l.min())\n",
    "\n",
    "        self.LR = learning_rate\n",
    "\n",
    "    def feed_forward(self, a):\n",
    "        '''Propagates an activation ``a`` from the input to the output of the network.'''\n",
    "        for b, w in zip(self.B, self.W):\n",
    "            print(w.shape, a.shape)\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "    \n",
    "    def learn(self, x, t):\n",
    "        '''NN learn when the weights and biases are modified to minimize the cost function.\n",
    "        To teach a NN, tuples at least a tuple ```(x, y)``` must be presented\n",
    "        to the NN, where ```x``` is a input training example (also called a feature vector)\n",
    "        and ``t``` is the associated (ideal) output (target) that the NN should learn.\n",
    "        \n",
    "        To modify the weights and the biases, Gradient Descend Optimization (GDO) is used.\n",
    "        '''\n",
    "        \n",
    "        # Find the gradient for each weight and bias of the NN\n",
    "        nabla_b, nabla_w = self.get_gradients(x, t)\n",
    "        \n",
    "        # w_ij^l -= \\alpha/len(x)\\nabla_w_ij^l\n",
    "        self.W = [w - (self.LR/len(x))*nw for w, nw in zip(self.W, nabla_w)]\n",
    "        \n",
    "        # b_i^l -= \\alpha/len(x)\\nabla_b_i^l\n",
    "        self.B = [b - (self.LR/len(x))*nb for b, nb in zip(self.B, nabla_b)]\n",
    "\n",
    "    def cost_derivative(self, A_at_L, target):\n",
    "        \"\"\"Derivative of the cost function: 1/2*(ideal_out - out)^2.\n",
    "        Notice that the scalar has been ignored (supposed to be 1) it will be\n",
    "        considerated to be a part of the learning rate used by GDO.\"\"\"\n",
    "        return (A_at_L - target)\n",
    "\n",
    "    def get_gradients(self, _in, ideal_out):\n",
    "        \"\"\" Given an input ``_in`` and an ideal output ``ideal_out``,\n",
    "        modify the weights and biases to minimize the cost of the error.\n",
    "        \n",
    "        Backpropagate the errors from the output to the first hidden layer,\n",
    "        and computes the \n",
    "        \"\"\"\n",
    "        #ipdb.set_trace() # <-------------------------- breakpoint\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.B`` and ``self.W``.\"\"\"\n",
    "        \n",
    "        # Returns the gradient of the cost function C(x) respect to the\n",
    "        # biases (nabla_b) and weights (nabla_w).\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.B]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.W]\n",
    "        #print(len(nabla_b), len(nabla_w))\n",
    "\n",
    "        # Forward pass. We compute two lists: activations and zs,\n",
    "        # with the activations and the z's of the neurons of the network.\n",
    "        A = _in\n",
    "        As = [_in] # list to store all the activations, layer by layer\n",
    "        #print(_in)\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.B, self.W):\n",
    "            #for i in activations:\n",
    "            #    print(i.shape)\n",
    "            z = np.dot(w, A) + b\n",
    "            zs.append(z)\n",
    "            A = sigmoid(z)\n",
    "            As.append(A)\n",
    "\n",
    "        # Backward pass. This is the backpropagation algorithm.\n",
    "        \n",
    "        # Derivative of the error of the cost function at the output (L-1) layer.\n",
    "        delta = self.cost_derivative(As[-1], ideal_out) * sigmoid_derivative(zs[-1])\n",
    "        \n",
    "        # The gradient of the cost function respect to the biases at the output layer\n",
    "        # is the calculus performed in the last sentence.\n",
    "        nabla_b[-1] = delta\n",
    "        \n",
    "        # The gradient of the cost function respect to the weights at the output\n",
    "        # is the previous derivative multiplied by the activations of the L-2 layer.\n",
    "        #print(delta.shape, As[-2].transpose().shape)\n",
    "        nabla_w[-1] = np.dot(delta, As[-2].transpose())\n",
    "\n",
    "        # Now, retropropagate the error of the cost function to the rest of the \n",
    "        # layers (starting at L-2) up to the first one (layer 0), computing the gradient.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l] # Negative indexes go backwards in the list\n",
    "            sp = sigmoid_derivative(z)\n",
    "            delta = np.dot(self.W[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, As[-l-1].transpose())\n",
    "\n",
    "        return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44fyb6sqp8mh"
   },
   "source": [
    "# Hyperparameters\n",
    "\n",
    "* Input layer: $s_0$ and $s_2$, samples of $s_0, s_1, s_2, \\cdots$. \n",
    "* Output layer: $\\hat{s}_1$, a prediction.\n",
    "* Initial prediction:\n",
    "$$\n",
    "\\hat{s}_1 = \\frac{s_0 + s_2}{2}.\n",
    "$$\n",
    "* $L$ layers and number of neurons by layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "wayPHdNqp8mj",
    "outputId": "dd3adf14-0567-4ca0-9f65-c2d0780b0d2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomizing biases using uniform [0.01, 0.99]\n",
      "(16, 2) (2, 1)\n",
      "(16, 16) (16, 1)\n",
      "(1, 16) (16, 1)\n",
      "[[130.]]\n"
     ]
    }
   ],
   "source": [
    "net = Network(sizes=[2, 16, 16, 1], initial_biases=-1)\n",
    "for i in range(2000):\n",
    "    net.learn(np.array([[10/255],[50/255]]), np.array([[130/255]]))\n",
    "print(net.feed_forward(np.array([[10/255],[50/255]])) * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "BsjTnBGLp8mv",
    "outputId": "bc985b94-dcfc-4eea-a8e1-9f385111bf31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All biases initialized to 0\n",
      "(16, 2) (2, 1)\n",
      "(16, 16) (16, 1)\n",
      "(1, 16) (16, 1)\n",
      "[[130.]]\n"
     ]
    }
   ],
   "source": [
    "net = Network(sizes=[2, 16, 16, 1], initial_biases=0)\n",
    "for i in range(2000):\n",
    "    net.learn(np.array([[10/255],[50/255]]), np.array([[130/255]]))\n",
    "print(net.feed_forward(np.array([[10/255],[50/255]])) * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "lqHwpEjGp8m5",
    "outputId": "2813f01b-ca5a-4950-e3c9-a8edb4573fa8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[37, 44, 42],\n",
       "       [38, 22,  3]])"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randint(low=0, high=100, size=(2,3))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "--2AC7z_p8nO",
    "outputId": "aa6de4f5-1405-4b47-9b0d-85ed9f7ab7ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AJnWKu8gp8nV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Copia de 1D.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
